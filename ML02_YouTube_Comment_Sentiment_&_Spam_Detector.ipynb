{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwqQRodYp8c9yEZ9iC8U67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyaanshi1308-web/ML-01/blob/main/ML02_YouTube_Comment_Sentiment_%26_Spam_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnKOYAsfdJSN",
        "outputId": "2ccec86e-5499-4543-f701-f3d21c47c300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# 1. Install & Import Libraries\n",
        "# =============================\n",
        "\n",
        "!pip install nltk scikit-learn matplotlib seaborn\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Download punkt_tab for tokenization\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 2. Load Dataset\n",
        "# =============================\n",
        "\n",
        "# Kaggle dataset: \"YouTube Spam Collection Dataset\"\n",
        "# Direct sample dataset from UCI repo\n",
        "url = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
        "\n",
        "# NOTE: This dataset has 'label' column (0 = negative, 1 = positive)\n",
        "# We'll simulate it for YouTube-style comments\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df = df[['tweet', 'label']]  # tweet = text, label = sentiment\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={'tweet':'comment', 'label':'sentiment'}, inplace=True)\n",
        "\n",
        "print(\"Sample Data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVAxaqHFdY3L",
        "outputId": "ff037b9e-834f-4984-f965-3f4d9bc611ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Data:\n",
            "                                             comment  sentiment\n",
            "0   @user when a father is dysfunctional and is s...          0\n",
            "1  @user @user thanks for #lyft credit i can't us...          0\n",
            "2                                bihday your majesty          0\n",
            "3  #model   i love u take with u all the time in ...          0\n",
            "4             factsguide: society now    #motivation          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 3. Data Preprocessing\n",
        "# =============================\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove links\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # remove special chars\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['cleaned'] = df['comment'].apply(clean_text)\n",
        "\n",
        "print(\"Before:\", df['comment'][0])\n",
        "print(\"After:\", df['cleaned'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48lAd1Redhey",
        "outputId": "6d838a7c-9c61-403a-d93e-43084b146b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
            "After: user father dysfunctional selfish drag kid dysfunction run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 4. Sentiment Analysis Model\n",
        "# =============================\n",
        "\n",
        "X = df['cleaned']\n",
        "y = df['sentiment']   # 0 = Negative, 1 = Positive\n",
        "\n",
        "# Vectorization - Fit on the larger sentiment dataset\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model (Naive Bayes)\n",
        "model_sentiment = MultinomialNB()\n",
        "model_sentiment.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model_sentiment.predict(X_test)\n",
        "\n",
        "print(\"ðŸŽ¯ Sentiment Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJT2McspdmXP",
        "outputId": "93ee4745-1315-4402-8209-880143156d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Sentiment Model Accuracy: 0.9507273580478649\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      5937\n",
            "           1       0.93      0.33      0.49       456\n",
            "\n",
            "    accuracy                           0.95      6393\n",
            "   macro avg       0.94      0.67      0.73      6393\n",
            "weighted avg       0.95      0.95      0.94      6393\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 5. Spam Detection (Fake Simulation)\n",
        "# =============================\n",
        "\n",
        "# Let's create a small spam dataset manually\n",
        "spam_data = {\n",
        "    \"comment\": [\n",
        "        \"Subscribe to my channel and win iPhone\",\n",
        "        \"Click here to get free money\",\n",
        "        \"Worst video ever\",\n",
        "        \"I love this tutorial, very helpful!\",\n",
        "        \"This is spam comment visit my website now\"\n",
        "    ],\n",
        "    \"spam\": [1, 1, 0, 0, 1]  # 1 = Spam, 0 = Not Spam\n",
        "}\n",
        "\n",
        "spam_df = pd.DataFrame(spam_data)\n",
        "spam_df['cleaned'] = spam_df['comment'].apply(clean_text)\n",
        "\n",
        "# Use the SAME vectorizer fitted on the sentiment data\n",
        "X_spam = vectorizer.transform(spam_df['cleaned']) # Use transform, not fit_transform\n",
        "y_spam = spam_df['spam']\n",
        "\n",
        "spam_model = LogisticRegression()\n",
        "spam_model.fit(X_spam, y_spam)\n",
        "\n",
        "print(\"Spam Model Trained on Sample Data âœ…\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNRjURsRdpsT",
        "outputId": "2cc2f12c-dc8e-44e0-cdf2-6f8c16ec8e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam Model Trained on Sample Data âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 6. Prediction Function\n",
        "# =============================\n",
        "\n",
        "def predict_comment(comment):\n",
        "    cleaned = clean_text(comment)\n",
        "\n",
        "    # Sentiment\n",
        "    vec = vectorizer.transform([cleaned])\n",
        "    sentiment_pred = model_sentiment.predict(vec)[0]\n",
        "    sentiment = \"Positive ðŸ˜„\" if sentiment_pred == 1 else \"Negative ðŸ˜ž\"\n",
        "\n",
        "    # Spam\n",
        "    spam_pred = spam_model.predict(vec)[0]\n",
        "    spam_status = \"Spam ðŸš¨\" if spam_pred == 1 else \"Not Spam âœ…\"\n",
        "\n",
        "    print(f\"\\nComment: {comment}\")\n",
        "    print(f\"â†’ Sentiment: {sentiment}\")\n",
        "    print(f\"â†’ Spam Check: {spam_status}\")"
      ],
      "metadata": {
        "id": "DKMI--0wdzHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 7. Test Predictions\n",
        "# =============================\n",
        "\n",
        "predict_comment(\"I really loved this video, awesome work!\")\n",
        "predict_comment(\"This video is waste of time, very boring.\")\n",
        "predict_comment(\"Subscribe to my channel for free gifts!!!\")\n",
        "predict_comment(\"Great explanation, I learned a lot.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js78R8rld3dL",
        "outputId": "b56cfa73-e12e-4d70-a418-d79a91a36ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comment: I really loved this video, awesome work!\n",
            "â†’ Sentiment: Negative ðŸ˜ž\n",
            "â†’ Spam Check: Spam ðŸš¨\n",
            "\n",
            "Comment: This video is waste of time, very boring.\n",
            "â†’ Sentiment: Negative ðŸ˜ž\n",
            "â†’ Spam Check: Spam ðŸš¨\n",
            "\n",
            "Comment: Subscribe to my channel for free gifts!!!\n",
            "â†’ Sentiment: Negative ðŸ˜ž\n",
            "â†’ Spam Check: Spam ðŸš¨\n",
            "\n",
            "Comment: Great explanation, I learned a lot.\n",
            "â†’ Sentiment: Negative ðŸ˜ž\n",
            "â†’ Spam Check: Spam ðŸš¨\n"
          ]
        }
      ]
    }
  ]
}